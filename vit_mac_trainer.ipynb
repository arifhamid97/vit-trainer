{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f77d0fe2-5d6c-4fe6-b09f-3098841824fc",
   "metadata": {},
   "source": [
    "# This notebook demonstrate how to finetune VIT model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8573546-f5ab-48d0-b77f-646a0877c4f4",
   "metadata": {},
   "source": [
    "## 1. Initial preperation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e3fe6e-33dc-4586-bbaf-5a03a9ac5f72",
   "metadata": {},
   "source": [
    "### 1.1 Install Depandacies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d33f4d5a-37e5-4a99-aeb7-10b5210dfa74",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: /Users/arifhamid/Desktop/portfolio/vit_trainer/.venv/bin/pip: bad interpreter: /Users/arifhamid/Desktop/portfolio/vit_training/.venv/bin/python: no such file or directory\n",
      "Requirement already satisfied: datasets in /Users/arifhamid/.pyenv/versions/3.10.5/lib/python3.10/site-packages (2.20.0)\n",
      "Requirement already satisfied: transformers in /Users/arifhamid/.pyenv/versions/3.10.5/lib/python3.10/site-packages (4.31.0)\n",
      "Requirement already satisfied: accelerate in /Users/arifhamid/.pyenv/versions/3.10.5/lib/python3.10/site-packages (0.31.0)\n",
      "Requirement already satisfied: torchvision in /Users/arifhamid/.pyenv/versions/3.10.5/lib/python3.10/site-packages (0.18.0)\n",
      "Requirement already satisfied: scikit-learn in /Users/arifhamid/.pyenv/versions/3.10.5/lib/python3.10/site-packages (1.4.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /Users/arifhamid/.pyenv/versions/3.10.5/lib/python3.10/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: filelock in /Users/arifhamid/.pyenv/versions/3.10.5/lib/python3.10/site-packages (from datasets) (3.12.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /Users/arifhamid/.pyenv/versions/3.10.5/lib/python3.10/site-packages (from datasets) (16.1.0)\n",
      "Requirement already satisfied: packaging in /Users/arifhamid/.pyenv/versions/3.10.5/lib/python3.10/site-packages (from datasets) (23.2)\n",
      "Requirement already satisfied: aiohttp in /Users/arifhamid/.pyenv/versions/3.10.5/lib/python3.10/site-packages (from datasets) (3.9.5)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /Users/arifhamid/.pyenv/versions/3.10.5/lib/python3.10/site-packages (from datasets) (4.66.4)\n",
      "Requirement already satisfied: xxhash in /Users/arifhamid/.pyenv/versions/3.10.5/lib/python3.10/site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /Users/arifhamid/.pyenv/versions/3.10.5/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/arifhamid/.pyenv/versions/3.10.5/lib/python3.10/site-packages (from datasets) (6.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/arifhamid/.pyenv/versions/3.10.5/lib/python3.10/site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.2 in /Users/arifhamid/.pyenv/versions/3.10.5/lib/python3.10/site-packages (from datasets) (0.23.4)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /Users/arifhamid/.pyenv/versions/3.10.5/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /Users/arifhamid/.pyenv/versions/3.10.5/lib/python3.10/site-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: fsspec[http]<=2024.5.0,>=2023.1.0 in /Users/arifhamid/.pyenv/versions/3.10.5/lib/python3.10/site-packages (from datasets) (2024.5.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in /Users/arifhamid/.pyenv/versions/3.10.5/lib/python3.10/site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/arifhamid/.pyenv/versions/3.10.5/lib/python3.10/site-packages (from transformers) (2024.5.15)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /Users/arifhamid/.pyenv/versions/3.10.5/lib/python3.10/site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /Users/arifhamid/.pyenv/versions/3.10.5/lib/python3.10/site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: torch>=1.10.0 in /Users/arifhamid/.pyenv/versions/3.10.5/lib/python3.10/site-packages (from accelerate) (2.3.0)\n",
      "Requirement already satisfied: psutil in /Users/arifhamid/.pyenv/versions/3.10.5/lib/python3.10/site-packages (from accelerate) (6.0.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/arifhamid/.pyenv/versions/3.10.5/lib/python3.10/site-packages (from torchvision) (10.3.0)\n",
      "Requirement already satisfied: sympy in /Users/arifhamid/.pyenv/versions/3.10.5/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.12)\n",
      "Requirement already satisfied: jinja2 in /Users/arifhamid/.pyenv/versions/3.10.5/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
      "Requirement already satisfied: networkx in /Users/arifhamid/.pyenv/versions/3.10.5/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.3)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/arifhamid/.pyenv/versions/3.10.5/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (4.11.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/arifhamid/.pyenv/versions/3.10.5/lib/python3.10/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/arifhamid/.pyenv/versions/3.10.5/lib/python3.10/site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /Users/arifhamid/.pyenv/versions/3.10.5/lib/python3.10/site-packages (from scikit-learn) (1.13.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/arifhamid/.pyenv/versions/3.10.5/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/arifhamid/.pyenv/versions/3.10.5/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/arifhamid/.pyenv/versions/3.10.5/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /Users/arifhamid/.pyenv/versions/3.10.5/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/arifhamid/.pyenv/versions/3.10.5/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/arifhamid/.pyenv/versions/3.10.5/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/arifhamid/.pyenv/versions/3.10.5/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/arifhamid/.pyenv/versions/3.10.5/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/arifhamid/.pyenv/versions/3.10.5/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/arifhamid/.pyenv/versions/3.10.5/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2023.5.7)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/arifhamid/.pyenv/versions/3.10.5/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/arifhamid/.pyenv/versions/3.10.5/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/arifhamid/.pyenv/versions/3.10.5/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/arifhamid/.pyenv/versions/3.10.5/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/arifhamid/.pyenv/versions/3.10.5/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/arifhamid/.pyenv/versions/3.10.5/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 24.1 is available.\n",
      "You should consider upgrading via the '/Users/arifhamid/.pyenv/versions/3.10.5/bin/python3.10 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "! pip install datasets transformers accelerate torchvision scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979f64e5-45d0-499f-9b5f-ab46982e70aa",
   "metadata": {},
   "source": [
    "### 1.2 Setup HF token. Need to setup HF token with write access because we will download foundation/base VIT model from HF hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e7a553-bf39-4796-ae50-19025f5145e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import interpreter_login\n",
    "\n",
    "interpreter_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9beaba5-1413-40a5-a2fa-6f0d6260ab6a",
   "metadata": {},
   "source": [
    "## 2. Data preparation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e43bbd6-ec44-466d-8901-2a7ce0cc17ab",
   "metadata": {},
   "source": [
    "### 2.1 Create custom data using ``dataset`` library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dbfd2c8-dd8d-46f0-9ce9-e2ae49fcce9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "LOCAL_DATASET_PATH = './Medicinal-plant-dataset'\n",
    "CACHE_DIR = './.hf_cache/'\n",
    "\n",
    "ds = load_dataset('imagefolder',data_dir=LOCAL_DATASET_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79930c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ds.train_test_split(test_size=0.2)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2765f917-89c5-49f9-ae26-b8fa1310aadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# push data to hf \n",
    "data.push_to_hub(\"funkepal/medicinal_plant_images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95bafe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from hf hub\n",
    "from datasets import load_dataset\n",
    "\n",
    "HF_DATASET_PATH = 'funkepal/medicinal_plant_images'\n",
    "CACHE_DIR = './.hf_cache/'\n",
    "\n",
    "hf_ds = load_dataset(HF_DATASET_PATH,cache_dir=CACHE_DIR)\n",
    "\n",
    "labels = hf_ds['train'].features['label']\n",
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6af0859-43e3-435f-b70f-703c060e8f25",
   "metadata": {},
   "source": [
    "## 3. Pre-processing Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8beca6ba-fbdb-4cb8-8b0f-d1c40d0c0682",
   "metadata": {},
   "source": [
    "### 3.1 Load pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc402b2e-9085-454d-806f-f1ddac631f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ViTImageProcessor\n",
    "\n",
    "model_name_or_path = 'google/vit-base-patch16-224-in21k'\n",
    "processor = ViTImageProcessor.from_pretrained(model_name_or_path,cache_dir=CACHE_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299f4822-68e4-45cd-a77a-415097b9a139",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e9e314-b83b-485e-9f50-6374476c2871",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This function will convert the image to tensor\n",
    "## image -> ViTImageProcessor -> tensor\n",
    "## because computer only understand number ViTImageProcessor act as tokenizer where it convert image to tensor\n",
    "def process_example(example):\n",
    "    inputs = processor(example['image'], return_tensors='pt')\n",
    "    inputs['labels'] = example['labels']\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef16a4c",
   "metadata": {},
   "source": [
    "### 3.2 Apply data augmentation to our image\n",
    " - this step make transformation to the image dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f046ef-d383-4ff3-b663-7e92337fe6d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from torchvision.transforms import (\n",
    "    CenterCrop,\n",
    "    Compose,\n",
    "    Normalize,\n",
    "    RandomHorizontalFlip,\n",
    "    RandomResizedCrop,\n",
    "    Resize,\n",
    "    ToTensor,\n",
    ")\n",
    "\n",
    "size =(processor.size['height'],processor.size['width'])\n",
    "\n",
    "normalize = Normalize(mean=processor.image_mean, std=processor.image_std)\n",
    "train_transforms = Compose(\n",
    "        [\n",
    "            RandomResizedCrop(size),\n",
    "            RandomHorizontalFlip(),\n",
    "            ToTensor(),\n",
    "            normalize,\n",
    "        ]\n",
    "    )\n",
    "\n",
    "val_transforms = Compose(\n",
    "        [\n",
    "            Resize(size),\n",
    "            CenterCrop(size),\n",
    "            ToTensor(),\n",
    "            normalize,\n",
    "        ]\n",
    "    )\n",
    "\n",
    "def preprocess_train(example_batch):\n",
    "    \"\"\"Apply train_transforms across a batch.\"\"\"\n",
    "    example_batch[\"pixel_values\"] = [train_transforms(image.convert(\"RGB\")) for image in example_batch[\"image\"]]\n",
    "    return example_batch\n",
    "\n",
    "def preprocess_val(example_batch):\n",
    "    \"\"\"Apply val_transforms across a batch.\"\"\"\n",
    "    example_batch[\"pixel_values\"] = [val_transforms(image.convert(\"RGB\")) for image in example_batch[\"image\"]]\n",
    "    return example_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8926e01f-15dd-4215-8dee-d0c5f1f62248",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split up training into training + validation\n",
    "train_ds = hf_ds['train']\n",
    "val_ds = hf_ds['test']\n",
    "\n",
    "train_ds.set_transform(preprocess_train)\n",
    "val_ds.set_transform(preprocess_val)\n",
    "\n",
    "train_ds[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d074bf66-56c7-4c97-b9dc-e9a66cbeb507",
   "metadata": {},
   "source": [
    "## 4. Training and Evaluation\n",
    "\n",
    "- Define a collate function.\n",
    "\n",
    "- Define an evaluation metric. During training, the model should be evaluated on its prediction accuracy. You should define a compute_metrics function accordingly.\n",
    "\n",
    "- Load a pretrained checkpoint\n",
    "\n",
    "- Define the training configuration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0f5f58-66b8-45ec-9626-9573e469815f",
   "metadata": {},
   "source": [
    "### 4.1 Define collate function\n",
    "- Batches are coming in as lists of dicts, so you can just unpack + stack those into batch tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e08cc3b-d4fd-4390-9dba-bf563b65c061",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return {\n",
    "        'pixel_values': torch.stack([x['pixel_values'] for x in batch]),\n",
    "        'labels': torch.tensor([x['label'] for x in batch])\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526583ff-6aab-49d6-92f4-6218d745895c",
   "metadata": {},
   "source": [
    "### 4.2 Define an evaluation metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9988a35-fa35-422a-b55a-81e62876cd39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datasets import load_metric\n",
    "\n",
    "metric = load_metric(\"accuracy\")\n",
    "def compute_metrics(p):\n",
    "    return metric.compute(predictions=np.argmax(p.predictions, axis=1), references=p.label_ids)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23dfd67-7e24-4187-9b6e-f0fb9d0930bc",
   "metadata": {},
   "source": [
    "### 4.3 Load a pretrained checkpoint\n",
    "- id2label and label2id mappings to have human-readable labels in the Hub widget (if you choose to push_to_hub)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c7d2b7-f480-4c1a-b5a6-8bc6c6731138",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ViTForImageClassification\n",
    "\n",
    "labels = hf_ds['train'].features['label'].names\n",
    "\n",
    "model = ViTForImageClassification.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    num_labels=len(labels),\n",
    "    id2label={str(i): c for i, c in enumerate(labels)},\n",
    "    label2id={c: str(i) for i, c in enumerate(labels)},\n",
    "    cache_dir=CACHE_DIR\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73cd9ab-e92d-45b1-bffc-d43ba562bfa6",
   "metadata": {},
   "source": [
    "### 4.4 Define the training configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35691b30-3b29-490e-81e3-4da68722599f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "import torch\n",
    "\n",
    "def check_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return 'cuda'\n",
    "    if torch.backends.mps.is_available():\n",
    "        return 'mps'\n",
    "    else:\n",
    "        return 'cpu'\n",
    "    \n",
    "device = torch.device(check_device())\n",
    "\n",
    "training_args_mac = TrainingArguments(\n",
    "  output_dir=\"./vit-medicinal-plant-finetune\",\n",
    "  per_device_train_batch_size=10,\n",
    "  eval_strategy=\"steps\",\n",
    "  num_train_epochs=1,\n",
    "  save_steps=200,\n",
    "  eval_steps=200,\n",
    "  logging_steps=10,\n",
    "  learning_rate=2e-4,\n",
    "  save_total_limit=2,\n",
    "  remove_unused_columns=False,\n",
    "  push_to_hub=False,\n",
    "  load_best_model_at_end=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46feb06b-02ed-4c8e-9dbe-55f10ebc9f78",
   "metadata": {},
   "source": [
    "### 4.5 Set our trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7d67f0-4e28-4066-9c96-f2a2e2844d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args_mac,\n",
    "    data_collator=collate_fn,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    tokenizer=processor,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62128588-c7d7-4539-aa8d-85efc28c0ed5",
   "metadata": {},
   "source": [
    "### 4.6 Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4587cfb3-00c8-4a64-83ab-4becc8d07897",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_results = trainer.train()\n",
    "trainer.save_model()\n",
    "trainer.log_metrics(\"train\", train_results.metrics)\n",
    "trainer.save_metrics(\"train\", train_results.metrics)\n",
    "trainer.save_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2322e4-1ddd-44c4-a1da-99717f991bee",
   "metadata": {},
   "source": [
    "### 4.7 Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2b99ef-e454-492d-babe-c6fe3d284285",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "metrics = trainer.evaluate(train_ds)\n",
    "trainer.log_metrics(\"eval\", metrics)\n",
    "trainer.save_metrics(\"eval\", metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6352d898-9885-415f-9178-d59a6d76f11d",
   "metadata": {},
   "source": [
    "### 4.8 Inference model from checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f69a4d-684f-4e18-b5c5-62eac7a4092d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## load model from checkpoint\n",
    "checkpoint_path = \"./vit-medicinal-plant-finetune/checkpoint-1900\"\n",
    "\n",
    "inference_processor = ViTImageProcessor.from_pretrained(checkpoint_path)\n",
    "inputs = inference_processor(val_ds[0]['image'], return_tensors=\"pt\")\n",
    "inference_model = ViTForImageClassification.from_pretrained(checkpoint_path)\n",
    "with torch.no_grad():\n",
    "    logits = inference_model(**inputs).logits\n",
    "\n",
    "predicted_label = logits.argmax(-1).item()\n",
    "predicted_class = inference_model.config.id2label[predicted_label]\n",
    "\n",
    "print(predicted_class)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba3bf6a-d520-4dbb-bd9c-b22416db4a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Converting model to pt format\n",
    "\n",
    "\n",
    "from transformers import ViTForImageClassification,ViTImageProcessor\n",
    "import torch\n",
    "\n",
    "## load model from checkpoint\n",
    "checkpoint_path = \"./vit-medicinal-plant-finetune/checkpoint-1900\"\n",
    "save_path = \"./torch_model\"\n",
    "inference_processor = ViTImageProcessor.from_pretrained(checkpoint_path)\n",
    "# inputs = inference_processor(val_ds[0]['image'], return_tensors=\"pt\")\n",
    "inference_model = ViTForImageClassification.from_pretrained(checkpoint_path)\n",
    "\n",
    "# torch.save(inference_model.state_dict(),f\"{save_path}/checkpoint-1900.pt\")\n",
    "for param_tensor in inference_model.state_dict():\n",
    "    print(param_tensor, \"\\t\", inference_model.state_dict()[param_tensor].size())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
