{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f77d0fe2-5d6c-4fe6-b09f-3098841824fc",
   "metadata": {},
   "source": [
    "# This notebook demonstrate how to finetune VIT model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8573546-f5ab-48d0-b77f-646a0877c4f4",
   "metadata": {},
   "source": [
    "## 1. Initial preperation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e3fe6e-33dc-4586-bbaf-5a03a9ac5f72",
   "metadata": {},
   "source": [
    "### 1.1 Install Depandacies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33f4d5a-37e5-4a99-aeb7-10b5210dfa74",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "! pip install datasets transformers accelerate matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979f64e5-45d0-499f-9b5f-ab46982e70aa",
   "metadata": {},
   "source": [
    "### 1.2 Setup HF token. Need to setup HF token with write access because we will download foundation/base VIT model from HF hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e7a553-bf39-4796-ae50-19025f5145e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import interpreter_login\n",
    "\n",
    "interpreter_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e891775-e280-4536-b705-1db50cb3f932",
   "metadata": {},
   "source": [
    "### 1.3 check device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7fa6ca9-36d0-469b-8a64-e2d9451ef357",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9beaba5-1413-40a5-a2fa-6f0d6260ab6a",
   "metadata": {},
   "source": [
    "## 2. Data preparation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e43bbd6-ec44-466d-8901-2a7ce0cc17ab",
   "metadata": {},
   "source": [
    "### 2.1 Load data from HF using ``dataset`` library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9058d2-2f38-48f4-8390-f3e6bdd7765a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "CACHE_DIR = './hf_cache'\n",
    "\n",
    "data = load_dataset(\"funkepal/medicinal_plant_images\",cache_dir = CACHE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dbfd2c8-dd8d-46f0-9ce9-e2ae49fcce9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6af0859-43e3-435f-b70f-703c060e8f25",
   "metadata": {},
   "source": [
    "## 3. Pre-processing Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8beca6ba-fbdb-4cb8-8b0f-d1c40d0c0682",
   "metadata": {},
   "source": [
    "### 3.1 Load pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc402b2e-9085-454d-806f-f1ddac631f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ViTImageProcessor\n",
    "\n",
    "model_name_or_path = 'google/vit-base-patch16-224-in21k'\n",
    "processor = ViTImageProcessor.from_pretrained(model_name_or_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299f4822-68e4-45cd-a77a-415097b9a139",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e9e314-b83b-485e-9f50-6374476c2871",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This function will convert the image to tensor\n",
    "## image -> ViTImageProcessor -> tensor\n",
    "## because computer only understand number ViTImageProcessor act as tokenizer where it convert image to tensor\n",
    "def process_example(example):\n",
    "    inputs = processor(example['image'], return_tensors='pt')\n",
    "    inputs['labels'] = example['labels']\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a04dc86-7d98-4116-84e1-a22d365dc069",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## This function accept batch of image and convert it to tensor\n",
    "# def transform(example_batch):\n",
    "#     # Take a list of PIL images and turn them to pixel values\n",
    "#     inputs = processor([x for x in example_batch['image']], return_tensors='pt')\n",
    "\n",
    "#     # Don't forget to include the labels!\n",
    "#     inputs['labels'] = example_batch['labels']\n",
    "#     print(inputs)\n",
    "#     return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f046ef-d383-4ff3-b663-7e92337fe6d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import (\n",
    "    CenterCrop,\n",
    "    Compose,\n",
    "    Normalize,\n",
    "    RandomHorizontalFlip,\n",
    "    RandomResizedCrop,\n",
    "    Resize,\n",
    "    ToTensor,\n",
    ")\n",
    "\n",
    "size =(processor.size['height'],processor.size['width'])\n",
    "\n",
    "normalize = Normalize(mean=processor.image_mean, std=processor.image_std)\n",
    "train_transforms = Compose(\n",
    "        [\n",
    "            RandomResizedCrop(size),\n",
    "            RandomHorizontalFlip(),\n",
    "            ToTensor(),\n",
    "            normalize,\n",
    "        ]\n",
    "    )\n",
    "\n",
    "val_transforms = Compose(\n",
    "        [\n",
    "            Resize(size),\n",
    "            CenterCrop(size),\n",
    "            ToTensor(),\n",
    "            normalize,\n",
    "        ]\n",
    "    )\n",
    "\n",
    "def preprocess_train(example_batch):\n",
    "    \"\"\"Apply train_transforms across a batch.\"\"\"\n",
    "    example_batch[\"pixel_values\"] = [train_transforms(image.convert(\"RGB\")) for image in example_batch[\"image\"]]\n",
    "    return example_batch\n",
    "\n",
    "def preprocess_val(example_batch):\n",
    "    \"\"\"Apply val_transforms across a batch.\"\"\"\n",
    "    example_batch[\"pixel_values\"] = [val_transforms(image.convert(\"RGB\")) for image in example_batch[\"image\"]]\n",
    "    return example_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8926e01f-15dd-4215-8dee-d0c5f1f62248",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split up training into training + validation\n",
    "train_ds = data['train']\n",
    "val_ds = data['test']\n",
    "\n",
    "train_ds.set_transform(preprocess_train)\n",
    "val_ds.set_transform(preprocess_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d074bf66-56c7-4c97-b9dc-e9a66cbeb507",
   "metadata": {},
   "source": [
    "## 4. Training and Evaluation\n",
    "\n",
    "- Define a collate function.\n",
    "\n",
    "- Define an evaluation metric. During training, the model should be evaluated on its prediction accuracy. You should define a compute_metrics function accordingly.\n",
    "\n",
    "- Load a pretrained checkpoint\n",
    "\n",
    "- Define the training configuration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0f5f58-66b8-45ec-9626-9573e469815f",
   "metadata": {},
   "source": [
    "### 4.1 Define collate function\n",
    "- Batches are coming in as lists of dicts, so you can just unpack + stack those into batch tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e08cc3b-d4fd-4390-9dba-bf563b65c061",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return {\n",
    "        'pixel_values': torch.stack([x['pixel_values'] for x in batch]),\n",
    "        'labels': torch.tensor([x['label'] for x in batch])\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526583ff-6aab-49d6-92f4-6218d745895c",
   "metadata": {},
   "source": [
    "### 4.2 Define an evaluation metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9988a35-fa35-422a-b55a-81e62876cd39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datasets import load_metric\n",
    "\n",
    "metric = load_metric(\"accuracy\")\n",
    "def compute_metrics(p):\n",
    "    return metric.compute(predictions=np.argmax(p.predictions, axis=1), references=p.label_ids)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23dfd67-7e24-4187-9b6e-f0fb9d0930bc",
   "metadata": {},
   "source": [
    "### 4.3 Load a pretrained checkpoint\n",
    "- id2label and label2id mappings to have human-readable labels in the Hub widget (if you choose to push_to_hub)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c7d2b7-f480-4c1a-b5a6-8bc6c6731138",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ViTForImageClassification\n",
    "\n",
    "labels = data['train'].features['label'].names\n",
    "\n",
    "model = ViTForImageClassification.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    num_labels=len(labels),\n",
    "    id2label={str(i): c for i, c in enumerate(labels)},\n",
    "    label2id={c: str(i) for i, c in enumerate(labels)}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73cd9ab-e92d-45b1-bffc-d43ba562bfa6",
   "metadata": {},
   "source": [
    "### 4.4 Define the training configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35691b30-3b29-490e-81e3-4da68722599f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "  output_dir=\"./vit-medicinal-plant-finetune-v2\",\n",
    "  per_device_train_batch_size=10,\n",
    "  eval_strategy=\"steps\",\n",
    "  num_train_epochs=10,\n",
    "  fp16=True,\n",
    "  save_steps=100,\n",
    "  eval_steps=100,\n",
    "  logging_steps=10,\n",
    "  learning_rate=2e-4,\n",
    "  save_total_limit=2,\n",
    "  remove_unused_columns=False,\n",
    "  push_to_hub=False,\n",
    "  load_best_model_at_end=True,\n",
    "\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46feb06b-02ed-4c8e-9dbe-55f10ebc9f78",
   "metadata": {},
   "source": [
    "### 4.5 Set our trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7d67f0-4e28-4066-9c96-f2a2e2844d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=collate_fn,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    tokenizer=processor,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62128588-c7d7-4539-aa8d-85efc28c0ed5",
   "metadata": {},
   "source": [
    "### 4.6 Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4587cfb3-00c8-4a64-83ab-4becc8d07897",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_results = trainer.train()\n",
    "trainer.save_model()\n",
    "trainer.log_metrics(\"train\", train_results.metrics)\n",
    "trainer.save_metrics(\"train\", train_results.metrics)\n",
    "trainer.save_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1896f1a9",
   "metadata": {},
   "source": [
    "### 4.7 Visualize some metric by hf trainer\n",
    "\n",
    "- during training with HF trainer you can get all the trainer log from ``trainer.state.log_history``\n",
    "- ``trainer.state.log_history`` contain all step and eval log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bdcee03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save log_history as pickel\n",
    "\n",
    "# import pickle\n",
    "\n",
    "# Example data\n",
    "# data = trainer.state.log_history\n",
    "# # Define the pickle file path\n",
    "# pickle_file = './output.pkl'\n",
    "\n",
    "# # Save data to pickle file\n",
    "# with open(pickle_file, 'wb') as f:\n",
    "#     pickle.dump(complex_data, f)\n",
    "\n",
    "# print(f'Data saved to {pickle_file}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6efbc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Open the .pkl file in read-binary mode\n",
    "with open('trainer_log_dump.pkl', 'rb') as file:\n",
    "    # Load the data from the file\n",
    "    trainer_log_dump = pickle.load(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446b9503-3c89-4e2f-9cb7-43fdbb9cdc1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualize some metric\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Sample data\n",
    "data = trainer_log_dump\n",
    "\n",
    "# Filter evaluation data\n",
    "eval_data = [entry for entry in data if 'eval_loss' in entry]\n",
    "\n",
    "# Extract metrics\n",
    "steps = [entry['step'] for entry in eval_data]\n",
    "eval_loss = [entry['eval_loss'] for entry in eval_data]\n",
    "eval_accuracy = [entry['eval_accuracy'] for entry in eval_data]\n",
    "\n",
    "# Plot eval_loss\n",
    "plt.style.use('dark_background')\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(steps, eval_loss, marker='.', linestyle='-', color='b', label='Eval Loss')\n",
    "plt.xlabel('Steps')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Evaluation Loss Over Steps')\n",
    "plt.legend()\n",
    "\n",
    "# Plot eval_accuracy\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(steps, eval_accuracy, marker='.', linestyle='-', color='g', label='Eval Accuracy')\n",
    "plt.xlabel('Steps')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Evaluation Accuracy Over Steps')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6cb8e1-1675-4e3b-8ed5-768ca19b1c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "data = trainer_log_dump\n",
    "\n",
    "# Filter non-evaluation data\n",
    "non_eval_data = [entry for entry in data if 'eval_loss' not in entry if 'loss' in entry if 'grad_norm' in entry if 'learning_rate' in entry]\n",
    "\n",
    "# Extract metrics\n",
    "steps = [entry['step'] for entry in non_eval_data]\n",
    "loss = [entry['loss'] for entry in non_eval_data]\n",
    "grad_norm = [entry['grad_norm'] for entry in non_eval_data ]\n",
    "learning_rate = [entry['learning_rate'] for entry in non_eval_data ]\n",
    "\n",
    "\n",
    "print(f\"{len(steps)}-{len(loss)}-{len(grad_norm)}-{len(learning_rate)}\")\n",
    "\n",
    "# Use dark background style\n",
    "plt.style.use('dark_background')\n",
    "\n",
    "# Plot loss\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(steps, loss, marker='.', linestyle='-', color='c', label='Loss')\n",
    "plt.xlabel('Steps')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Over Steps')\n",
    "plt.legend()\n",
    "\n",
    "# Plot grad_norm\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(steps, grad_norm, marker='.', linestyle='-', color='m', label='Grad Norm')\n",
    "plt.xlabel('Steps')\n",
    "plt.ylabel('Grad Norm')\n",
    "plt.title('Gradient Norm Over Steps')\n",
    "plt.legend()\n",
    "\n",
    "# Plot learning_rate\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(steps, learning_rate, marker='.', linestyle='-', color='y', label='Learning Rate')\n",
    "plt.xlabel('Steps')\n",
    "plt.ylabel('Learning Rate')\n",
    "plt.title('Learning Rate Over Steps')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2322e4-1ddd-44c4-a1da-99717f991bee",
   "metadata": {},
   "source": [
    "### 4.7 Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2b99ef-e454-492d-babe-c6fe3d284285",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "metrics = trainer.evaluate(train_ds)\n",
    "trainer.log_metrics(\"eval\", metrics)\n",
    "trainer.save_metrics(\"eval\", metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6352d898-9885-415f-9178-d59a6d76f11d",
   "metadata": {},
   "source": [
    "### 4.8 Inference model from checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f69a4d-684f-4e18-b5c5-62eac7a4092d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## load model from checkpoint\n",
    "checkpoint_path = \"./vit-medicinal-plant-finetune-v2/checkpoint-4800\"\n",
    "\n",
    "inference_processor = ViTImageProcessor.from_pretrained(checkpoint_path)\n",
    "inputs = inference_processor(val_ds[0]['image'], return_tensors=\"pt\")\n",
    "inference_model = ViTForImageClassification.from_pretrained(checkpoint_path)\n",
    "with torch.no_grad():\n",
    "    logits = inference_model(**inputs).logits\n",
    "\n",
    "predicted_label = logits.argmax(-1).item()\n",
    "predicted_class = inference_model.config.id2label[predicted_label]\n",
    "\n",
    "print(predicted_class)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba3bf6a-d520-4dbb-bd9c-b22416db4a6a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Converting model to pt format\n",
    "\n",
    "\n",
    "from transformers import ViTForImageClassification,ViTImageProcessor\n",
    "import torch\n",
    "\n",
    "## load model from checkpoint\n",
    "checkpoint_path = \"./vit-medicinal-plant-finetune/checkpoint-1900\"\n",
    "save_path = \"./torch_model\"\n",
    "inference_processor = ViTImageProcessor.from_pretrained(checkpoint_path)\n",
    "# inputs = inference_processor(val_ds[0]['image'], return_tensors=\"pt\")\n",
    "inference_model = ViTForImageClassification.from_pretrained(checkpoint_path)\n",
    "\n",
    "# torch.save(inference_model.state_dict(),f\"{save_path}/checkpoint-1900.pt\")\n",
    "for param_tensor in inference_model.state_dict():\n",
    "    print(param_tensor, \"\\t\", inference_model.state_dict()[param_tensor].size())\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "57ba75c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nooni\n"
     ]
    }
   ],
   "source": [
    "## load model from hub\n",
    "\n",
    "from transformers import ViTImageProcessor,ViTForImageClassification\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "checkpoint_path = \"funkepal/vit-medicinal-plant-finetune\"\n",
    "CACHE_DIR = './.hf_cache'\n",
    "\n",
    "inference_processor = ViTImageProcessor.from_pretrained(checkpoint_path,cache_dir=CACHE_DIR)\n",
    "inference_model = ViTForImageClassification.from_pretrained(checkpoint_path,cache_dir=CACHE_DIR)\n",
    "\n",
    "\n",
    "sample_image = Image.open('1.jpg')\n",
    "inputs = inference_processor(sample_image, return_tensors = 'pt')\n",
    "with torch.no_grad():\n",
    "    logits = inference_model(**inputs).logits\n",
    "\n",
    "predicted_label = logits.argmax(-1).item()\n",
    "predicted_class = inference_model.config.id2label[predicted_label]\n",
    "\n",
    "print(predicted_class)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
